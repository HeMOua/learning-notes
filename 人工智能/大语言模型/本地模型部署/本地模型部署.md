# 本地模型部署

## 常见框架

本地模型部署框架如下：

| 模型部署框架             | Xinference                                                                               | LocalAI                                                    | Ollama                                                                         | FastChat                                                                             |
|--------------------|------------------------------------------------------------------------------------------|------------------------------------------------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|
| OpenAI API 接口对齐    | ✅                                                                                        | ✅                                                          | ✅                                                                              | ✅                                                                                    |
| 加速推理引擎             | GPTQ, GGML, vLLM, TensorRT, mlx                                                          | GPTQ, GGML, vLLM, TensorRT                                 | GGUF, GGML                                                                     | vLLM                                                                                 |
| 接入模型类型             | LLM, Embedding, Rerank, Text-to-Image, Vision, Audio                                     | LLM, Embedding, Rerank, Text-to-Image, Vision, Audio       | LLM, Text-to-Image, Vision                                                     | LLM, Vision                                                                          |
| Function Call      | ✅                                                                                        | ✅                                                          | ✅                                                                              | /                                                                                    |
| 更多平台支持(CPU, Metal) | ✅                                                                                        | ✅                                                          | ✅                                                                              | ✅                                                                                    |
| 异构                 | ✅                                                                                        | ✅                                                          | /                                                                              | /                                                                                    |
| 集群                 | ✅                                                                                        | ✅                                                          | /                                                                              | /                                                                                    |
| 操作文档链接             | [Xinference 文档](https://inference.readthedocs.io/zh-cn/latest/models/builtin/index.html) | [LocalAI 文档](https://localai.io/model-compatibility/)      | [Ollama 文档](https://github.com/ollama/ollama?tab=readme-ov-file#model-library) | [FastChat 文档](https://github.com/lm-sys/FastChat#install)                            |
| 可用模型               | [Xinference 已支持模型](https://inference.readthedocs.io/en/latest/models/builtin/index.html) | [LocalAI 已支持模型](https://localai.io/model-compatibility/#/) | [Ollama 已支持模型](https://ollama.com/library#/)                                   | [FastChat 已支持模型](https://github.com/lm-sys/FastChat/blob/main/docs/model_support.md) |

## 加速推理引擎

1. **GPTQ (Quantized Transformer Quantization)**

GPTQ是一种量化方法，主要用于将大型Transformer模型（例如GPT模型）进行参数量化，从而减少内存占用和加速推理。GPTQ通常通过==把模型权重从高精度（如FP32）减少到较低精度（如INT8）==，来在推理过程中保持相似的精度表现，但显著降低了计算和存储成本。

2. **GGML (Geospatial Gaussian Mixture Library)**

GGML是一个轻量化的库，专门用于在CPU上运行大型语言模型，通常应用于推理阶段。它基于==量化权重==来使得模型在低端硬件上运行，从而使得复杂模型在资源受限的环境中部署成为可能。GGML支持对模型进行不同程度的量化以优化内存占用，并且==适合嵌入式设备和无GPU环境==。

3. **vLLM (virtual LLM)**

vLLM是一种优化大型语言模型推理速度的技术，通常==结合了内存管理和计算图优化来实现高效推理==。vLLM通过管理内存的动态分配和释放，减少不必要的计算，以及使用缓存技术以加速推理，特别==适合在有限资源的环境下执行高效的大规模模型推理==。

4. **TensorRT**

TensorRT是NVIDIA开发的一个深度学习推理优化库，专为在NVIDIA GPU上加速深度学习模型的推理性能。它通过一系列优化，例如内核融合、精度量化（FP16/INT8），以及图优化，将训练好的模型转换为高效的推理引擎。==TensorRT适用于需要低延迟和高吞吐量的场景，特别是在NVIDIA GPU硬件上进行推理部署==。

5. **MLX (Model Loader for Exchange)**

MLX通常指一种用于跨平台和跨框架加载深度学习模型的工具或协议。它的目标是使不同框架的模型更容易在不同环境中部署和使用。通过标准化模型的输入/输出和格式，==MLX减少了在不同平台之间迁移模型的复杂性==，使得开发者可以方便地利用不同硬件和软件平台的优势。

6. **GGUF (Grok GGML Unified Format)**

GGUF是GGML模型的一种新格式，专门设计用于在不同平台上实现统一的模型加载和推理体验。它的目标是解决模型格式的兼容性问题，使得使用者可以==跨平台部署相同模型==，而无需对模型格式进行大量的手动调整。GGUF集成了一些优化，特别适合在轻量级环境下的推理任务。